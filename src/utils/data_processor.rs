//! Data processor for Unihan database files

// Note: ScriptMapping types removed as we now use simple HashMap<String, String> for clean data
use crate::utils::unicode_utils::code_point_to_char;
use serde_json;
use std::collections::{HashMap, HashSet};
use std::fs;
use std::fs::File;
use std::io::{BufRead, BufReader};

/// Data file path constants to avoid repetition
mod paths {
    pub const SCRIPT_DIR: &str = "data/processed/script_conversion";
    pub const NORM_DIR: &str = "data/processed/normalization";

    pub const T2S_MAPPINGS: &str =
        "data/processed/script_conversion/traditional_to_simplified.json";
    pub const S2T_MAPPINGS: &str =
        "data/processed/script_conversion/simplified_to_traditional.json";
    pub const SCRIPT_STATS: &str = "data/processed/script_conversion/script_conversion_stats.json";

    pub const SEMANTIC_VARIANTS: &str = "data/processed/normalization/semantic_variants.json";
    pub const COMPAT_VARIANTS: &str = "data/processed/normalization/compatibility_variants.json";
    pub const KANGXI_RADICALS: &str = "data/processed/normalization/kangxi_radicals.json";
    pub const NORM_STATS: &str = "data/processed/normalization/normalization_stats.json";

    pub const UNIHAN_IRG: &str = "Unihan/Unihan_IRGSources.txt";
}

/// Processor for Unihan database files
pub struct UnihanDataProcessor;

impl UnihanDataProcessor {
    /// Process all Unihan files and generate clean separated mappings
    pub fn process_all() -> Result<(), Box<dyn std::error::Error>> {
        let processor = Self;
        println!("ğŸš€ Starting clean data generation with proper separation...");

        // Step 1: Process script conversion mappings (Traditional â†” Simplified)
        println!("\nğŸ“‹ Step 1: Processing script conversion mappings...");
        processor.process_script_conversion_mappings("Unihan/Unihan_Variants.txt")?;

        // Step 2: Process normalization mappings (variants, compatibility, etc.)
        // EXCLUDING pairs that already exist in script conversion
        println!("\nğŸ“‹ Step 2: Processing normalization mappings...");
        processor.process_normalization_mappings(
            "Unihan/Unihan_Variants.txt",
            "Unihan/Unihan_IRGSources.txt",
        )?;

        println!("\nâœ… Clean data generation completed!");
        Ok(())
    }

    /// Process script conversion mappings (Traditional â†” Simplified) from kSimplifiedVariant and kTraditionalVariant
    fn process_script_conversion_mappings(
        &self,
        path: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        // Collect raw relationships
        let mut t2s_mappings: HashMap<String, String> = HashMap::new();
        let mut s2t_mappings: HashMap<String, String> = HashMap::new();
        let mut processed_pairs: HashSet<(String, String)> = HashSet::new();

        let file = File::open(path)?;
        let reader = BufReader::new(file);

        for line in reader.lines() {
            let line = line?;
            if line.starts_with('#') || line.trim().is_empty() {
                continue;
            }

            let parts: Vec<&str> = line.split('\t').collect();
            if parts.len() < 3 {
                continue;
            }

            let source_cp = parts[0];
            let kind = parts[1];
            let targets_raw = parts[2];

            let Some(source_char) = code_point_to_char(source_cp) else {
                continue;
            };

            match kind {
                "kSimplifiedVariant" => {
                    // A kSimplifiedVariant B => A (traditional) â†’ B (simplified)
                    if let Some(first) = targets_raw.split_whitespace().next() {
                        if let Some(target_char) = code_point_to_char(first) {
                            let trad = source_char.to_string();
                            let simp = target_char.to_string();

                            if !processed_pairs.contains(&(trad.clone(), simp.clone())) {
                                t2s_mappings.insert(trad.clone(), simp.clone());
                                processed_pairs.insert((trad, simp));
                            }
                        }
                    }
                }
                "kTraditionalVariant" => {
                    // A kTraditionalVariant B => A (simplified) â†’ B (traditional)
                    // Only take the first one to avoid ambiguity
                    if let Some(first) = targets_raw.split_whitespace().next() {
                        let clean = first.split('<').next().unwrap_or(first);
                        if let Some(target_char) = code_point_to_char(clean) {
                            let simp = source_char.to_string();
                            let trad = target_char.to_string();

                            if !processed_pairs.contains(&(trad.clone(), simp.clone())) {
                                s2t_mappings.insert(simp.clone(), trad.clone());
                                processed_pairs.insert((trad, simp));
                            }
                        }
                    }
                }
                _ => {}
            }
        }

        // Save Traditional â†’ Simplified mappings
        let t2s_path = paths::T2S_MAPPINGS;
        let t2s_json = serde_json::to_string_pretty(&t2s_mappings)?;
        fs::write(t2s_path, t2s_json)?;
        println!(
            "âœ… Saved {} Traditionalâ†’Simplified mappings to: {}",
            t2s_mappings.len(),
            t2s_path
        );

        // Save Simplified â†’ Traditional mappings
        let s2t_path = paths::S2T_MAPPINGS;
        let s2t_json = serde_json::to_string_pretty(&s2t_mappings)?;
        fs::write(s2t_path, s2t_json)?;
        println!(
            "âœ… Saved {} Simplifiedâ†’Traditional mappings to: {}",
            s2t_mappings.len(),
            s2t_path
        );

        // Save statistics
        let stats = serde_json::json!({
            "traditional_to_simplified_count": t2s_mappings.len(),
            "simplified_to_traditional_count": s2t_mappings.len(),
            "total_script_conversion_pairs": processed_pairs.len(),
            "generation_timestamp": std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs()
        });
        let stats_path = paths::SCRIPT_STATS;
        fs::write(stats_path, serde_json::to_string_pretty(&stats)?)?;
        println!("âœ… Saved statistics to: {}", stats_path);

        Ok(())
    }

    /// Process normalization mappings (variants â†’ standard forms) EXCLUDING script conversion pairs
    fn process_normalization_mappings(
        &self,
        variants_path: &str,
        irg_path: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        // Step 1: Load existing script conversion pairs to exclude them
        let script_pairs = self.load_script_conversion_pairs()?;
        println!(
            "ğŸ“‹ Loaded {} script conversion pairs to exclude",
            script_pairs.len()
        );

        // Step 2: Process semantic variants
        let semantic_variants =
            self.process_semantic_variants_clean(variants_path, &script_pairs)?;

        // Step 3: Process compatibility variants
        let compatibility_variants =
            self.process_compatibility_variants_clean(irg_path, &script_pairs)?;

        // Step 4: Process Kangxi radicals
        let kangxi_variants = self.process_kangxi_radicals_clean(&script_pairs)?;

        // Step 5: Save normalization statistics
        let stats = serde_json::json!({
            "semantic_variants_count": semantic_variants.len(),
            "compatibility_variants_count": compatibility_variants.len(),
            "kangxi_radicals_count": kangxi_variants.len(),
            "total_normalization_mappings": semantic_variants.len() + compatibility_variants.len() + kangxi_variants.len(),
            "excluded_script_pairs": script_pairs.len(),
            "generation_timestamp": std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs()
        });
        let stats_path = "data/processed/normalization/normalization_stats.json";
        fs::write(stats_path, serde_json::to_string_pretty(&stats)?)?;
        println!("âœ… Saved normalization statistics to: {}", stats_path);

        Ok(())
    }

    /// Load script conversion pairs to exclude from normalization
    fn load_script_conversion_pairs(
        &self,
    ) -> Result<HashSet<(String, String)>, Box<dyn std::error::Error>> {
        let mut pairs = HashSet::new();

        // Load Traditional â†’ Simplified
        if let Ok(contents) =
            fs::read_to_string("data/processed/script_conversion/traditional_to_simplified.json")
        {
            if let Ok(mappings) = serde_json::from_str::<HashMap<String, String>>(&contents) {
                for (trad, simp) in mappings {
                    pairs.insert((trad, simp));
                }
            }
        }

        // Load Simplified â†’ Traditional
        if let Ok(contents) =
            fs::read_to_string("data/processed/script_conversion/simplified_to_traditional.json")
        {
            if let Ok(mappings) = serde_json::from_str::<HashMap<String, String>>(&contents) {
                for (simp, trad) in mappings {
                    pairs.insert((trad, simp));
                }
            }
        }

        Ok(pairs)
    }

    /// Process semantic variants with proper standard form detection, excluding script pairs
    fn process_semantic_variants_clean(
        &self,
        path: &str,
        script_pairs: &HashSet<(String, String)>,
    ) -> Result<HashMap<String, String>, Box<dyn std::error::Error>> {
        let mut semantic_mappings = HashMap::new();
        let file = File::open(path)?;
        let reader = BufReader::new(file);

        for line in reader.lines() {
            let line = line?;
            if line.starts_with('#') || line.trim().is_empty() {
                continue;
            }

            let parts: Vec<&str> = line.split('\t').collect();
            if parts.len() < 3 || parts[1] != "kSemanticVariant" {
                continue;
            }

            let source_cp = parts[0];
            let targets_raw = parts[2];

            let Some(source_char) = code_point_to_char(source_cp) else {
                continue;
            };

            // Process each target
            for target_raw in targets_raw.split_whitespace() {
                let clean_target = target_raw.split('<').next().unwrap_or(target_raw);
                if let Some(target_char) = code_point_to_char(clean_target) {
                    let source_str = source_char.to_string();
                    let target_str = target_char.to_string();

                    // Skip if this pair is already handled by script conversion
                    if script_pairs.contains(&(source_str.clone(), target_str.clone()))
                        || script_pairs.contains(&(target_str.clone(), source_str.clone()))
                    {
                        continue;
                    }

                    // Determine standard form using Unicode block priority
                    if let Some((variant, standard)) =
                        self.determine_standard_form(source_char, target_char)
                    {
                        semantic_mappings.insert(variant.to_string(), standard.to_string());
                    }
                }
            }
        }

        // Save semantic variants
        let path = "data/processed/normalization/semantic_variants.json";
        let json = serde_json::to_string_pretty(&semantic_mappings)?;
        fs::write(path, json)?;
        println!(
            "âœ… Saved {} semantic variant mappings to: {}",
            semantic_mappings.len(),
            path
        );

        Ok(semantic_mappings)
    }

    /// Determine which character is the standard form
    fn determine_standard_form(&self, char1: char, char2: char) -> Option<(char, char)> {
        let code1 = char1 as u32;
        let code2 = char2 as u32;

        // Primary rule: Main CJK block (U+4E00-U+9FFF) is preferred over compatibility blocks
        let is_main_1 = code1 >= 0x4E00 && code1 <= 0x9FFF;
        let is_main_2 = code2 >= 0x4E00 && code2 <= 0x9FFF;

        match (is_main_1, is_main_2) {
            (true, false) => Some((char2, char1)), // compatibility â†’ main
            (false, true) => Some((char1, char2)), // compatibility â†’ main
            (true, true) => {
                // Both in main CJK: use kIICore region count to determine standard form
                let iicore1 = self.get_iicore_count(char1);
                let iicore2 = self.get_iicore_count(char2);

                if iicore1 > iicore2 {
                    Some((char2, char1)) // variant â†’ standard (char1 is more standard)
                } else if iicore2 > iicore1 {
                    Some((char1, char2)) // variant â†’ standard (char2 is more standard)
                } else {
                    // Equal or no kIICore data - skip ambiguous cases
                    None
                }
            }
            (false, false) => None, // Both in compatibility blocks - skip
        }
    }

    /// Get kIICore region count for a character (higher count = more standard)
    fn get_iicore_count(&self, ch: char) -> usize {
        let code_point = format!("U+{:04X}", ch as u32);

        // Try to read from Unihan IRG Sources file
        if let Ok(contents) = fs::read_to_string("Unihan/Unihan_IRGSources.txt") {
            for line in contents.lines() {
                if line.starts_with(&code_point) && line.contains("kIICore") {
                    // Extract kIICore value: "U+4E00  kIICore AGTJHKMP"
                    if let Some(iicore_part) = line.split("kIICore").nth(1) {
                        let iicore_regions = iicore_part.trim();
                        return iicore_regions.len(); // Each letter = one region
                    }
                }
            }
        }

        0 // No kIICore data found
    }

    /// Process compatibility variants excluding script pairs
    fn process_compatibility_variants_clean(
        &self,
        path: &str,
        script_pairs: &HashSet<(String, String)>,
    ) -> Result<HashMap<String, String>, Box<dyn std::error::Error>> {
        let mut compatibility_mappings = HashMap::new();
        let file = File::open(path)?;
        let reader = BufReader::new(file);

        for line in reader.lines() {
            let line = line?;
            if line.starts_with('#') || line.trim().is_empty() {
                continue;
            }

            let parts: Vec<&str> = line.split('\t').collect();
            if parts.len() < 3 || parts[1] != "kCompatibilityVariant" {
                continue;
            }

            let source_cp = parts[0];
            let targets_raw = parts[2];

            let Some(source_char) = code_point_to_char(source_cp) else {
                continue;
            };

            if let Some(target_raw) = targets_raw.split_whitespace().next() {
                let clean_target = target_raw.split('<').next().unwrap_or(target_raw);
                if let Some(target_char) = code_point_to_char(clean_target) {
                    let source_str = source_char.to_string();
                    let target_str = target_char.to_string();

                    // Skip if this pair is already handled by script conversion
                    if script_pairs.contains(&(source_str.clone(), target_str.clone()))
                        || script_pairs.contains(&(target_str.clone(), source_str.clone()))
                    {
                        continue;
                    }

                    // For compatibility variants, source is always the variant
                    compatibility_mappings.insert(source_str, target_str);
                }
            }
        }

        // Save compatibility variants
        let path = "data/processed/normalization/compatibility_variants.json";
        let json = serde_json::to_string_pretty(&compatibility_mappings)?;
        fs::write(path, json)?;
        println!(
            "âœ… Saved {} compatibility variant mappings to: {}",
            compatibility_mappings.len(),
            path
        );

        Ok(compatibility_mappings)
    }

    /// Process Kangxi radicals excluding script pairs
    fn process_kangxi_radicals_clean(
        &self,
        script_pairs: &HashSet<(String, String)>,
    ) -> Result<HashMap<String, String>, Box<dyn std::error::Error>> {
        let mut kangxi_mappings = HashMap::new();

        // Hardcoded Kangxi radical mappings (these are fixed Unicode assignments)
        let kangxi_data = [
            (0x2F00, 'ä¸€'),
            (0x2F01, 'ä¸¨'),
            (0x2F02, 'ä¸¶'),
            (0x2F03, 'ä¸¿'),
            (0x2F04, 'ä¹™'),
            (0x2F05, 'äº…'),
            (0x2F06, 'äºŒ'),
            (0x2F07, 'äº '),
            (0x2F08, 'äºº'),
            (0x2F09, 'å„¿'),
            (0x2F0A, 'å…¥'),
            (0x2F0B, 'å…«'),
            (0x2F0C, 'å†‚'),
            (0x2F0D, 'å†–'),
            (0x2F0E, 'å†«'),
            (0x2F0F, 'å‡ '),
            (0x2F10, 'å‡µ'),
            (0x2F11, 'åˆ€'),
            (0x2F12, 'åŠ›'),
            (0x2F13, 'å‹¹'),
            (0x2F14, 'åŒ•'),
            (0x2F15, 'åŒš'),
            (0x2F16, 'åŒ¸'),
            (0x2F17, 'å'),
            (0x2F18, 'åœ'),
            (0x2F19, 'å©'),
            (0x2F1A, 'å‚'),
            (0x2F1B, 'å¶'),
            (0x2F1C, 'åˆ'),
            (0x2F1D, 'å£'),
            (0x2F1E, 'å›—'),
            (0x2F1F, 'åœŸ'),
            (0x2F20, 'å£«'),
            (0x2F21, 'å¤‚'),
            (0x2F22, 'å¤Š'),
            (0x2F23, 'å¤•'),
            (0x2F24, 'å¤§'),
            (0x2F25, 'å¥³'),
            (0x2F26, 'å­'),
            (0x2F27, 'å®€'),
            (0x2F28, 'å¯¸'),
            (0x2F29, 'å°'),
            (0x2F2A, 'å°¢'),
            (0x2F2B, 'å°¸'),
            (0x2F2C, 'å±®'),
            (0x2F2D, 'å±±'),
            (0x2F2E, 'å·›'),
            (0x2F2F, 'å·¥'),
            (0x2F30, 'å·±'),
            (0x2F31, 'å·¾'),
            (0x2F32, 'å¹²'),
            (0x2F33, 'å¹º'),
            (0x2F34, 'å¹¿'),
            (0x2F35, 'å»´'),
            (0x2F36, 'å»¾'),
            (0x2F37, 'å¼‹'),
            (0x2F38, 'å¼“'),
            (0x2F39, 'å½'),
            (0x2F3A, 'å½¡'),
            (0x2F3B, 'å½³'),
            (0x2F3C, 'å¿ƒ'),
            (0x2F3D, 'æˆˆ'),
            (0x2F3E, 'æˆ¶'),
            (0x2F3F, 'æ‰‹'),
            (0x2F40, 'æ”¯'),
            (0x2F41, 'æ”´'),
            (0x2F42, 'æ–‡'),
            (0x2F43, 'æ–—'),
            (0x2F44, 'æ–¤'),
            (0x2F45, 'æ–¹'),
            (0x2F46, 'æ— '),
            (0x2F47, 'æ—¥'),
            (0x2F48, 'æ›°'),
            (0x2F49, 'æœˆ'),
            (0x2F4A, 'æœ¨'),
            (0x2F4B, 'æ¬ '),
            (0x2F4C, 'æ­¢'),
            (0x2F4D, 'æ­¹'),
            (0x2F4E, 'æ®³'),
            (0x2F4F, 'æ¯‹'),
            (0x2F50, 'æ¯”'),
            (0x2F51, 'æ¯›'),
            (0x2F52, 'æ°'),
            (0x2F53, 'æ°”'),
            (0x2F54, 'æ°´'),
            (0x2F55, 'ç«'),
            (0x2F56, 'çˆª'),
            (0x2F57, 'çˆ¶'),
            (0x2F58, 'çˆ»'),
            (0x2F59, 'çˆ¿'),
            (0x2F5A, 'ç‰‡'),
            (0x2F5B, 'ç‰™'),
            (0x2F5C, 'ç‰›'),
            (0x2F5D, 'çŠ¬'),
            (0x2F5E, 'ç„'),
            (0x2F5F, 'ç‰'),
            (0x2F60, 'ç“œ'),
            (0x2F61, 'ç“¦'),
            (0x2F62, 'ç”˜'),
            (0x2F63, 'ç”Ÿ'),
            (0x2F64, 'ç”¨'),
            (0x2F65, 'ç”°'),
            (0x2F66, 'ç–‹'),
            (0x2F67, 'ç–’'),
            (0x2F68, 'ç™¶'),
            (0x2F69, 'ç™½'),
            (0x2F6A, 'çš®'),
            (0x2F6B, 'çš¿'),
            (0x2F6C, 'ç›®'),
            (0x2F6D, 'çŸ›'),
            (0x2F6E, 'çŸ¢'),
            (0x2F6F, 'çŸ³'),
            (0x2F70, 'ç¤º'),
            (0x2F71, 'ç¦¸'),
            (0x2F72, 'ç¦¾'),
            (0x2F73, 'ç©´'),
            (0x2F74, 'ç«‹'),
            (0x2F75, 'ç«¹'),
            (0x2F76, 'ç±³'),
            (0x2F77, 'ç³¸'),
            (0x2F78, 'ç¼¶'),
            (0x2F79, 'ç½‘'),
            (0x2F7A, 'ç¾Š'),
            (0x2F7B, 'ç¾½'),
            (0x2F7C, 'è€'),
            (0x2F7D, 'è€Œ'),
            (0x2F7E, 'è€’'),
            (0x2F7F, 'è€³'),
            (0x2F80, 'è¿'),
            (0x2F81, 'è‚‰'),
            (0x2F82, 'è‡£'),
            (0x2F83, 'è‡ª'),
            (0x2F84, 'è‡³'),
            (0x2F85, 'è‡¼'),
            (0x2F86, 'èˆŒ'),
            (0x2F87, 'èˆ›'),
            (0x2F88, 'èˆŸ'),
            (0x2F89, 'è‰®'),
            (0x2F8A, 'è‰²'),
            (0x2F8B, 'è‰¸'),
            (0x2F8C, 'è™'),
            (0x2F8D, 'è™«'),
            (0x2F8E, 'è¡€'),
            (0x2F8F, 'è¡Œ'),
            (0x2F90, 'è¡£'),
            (0x2F91, 'è¥¾'),
            (0x2F92, 'è¦‹'),
            (0x2F93, 'è§’'),
            (0x2F94, 'è¨€'),
            (0x2F95, 'è°·'),
            (0x2F96, 'è±†'),
            (0x2F97, 'è±•'),
            (0x2F98, 'è±¸'),
            (0x2F99, 'è²'),
            (0x2F9A, 'èµ¤'),
            (0x2F9B, 'èµ°'),
            (0x2F9C, 'è¶³'),
            (0x2F9D, 'èº«'),
            (0x2F9E, 'è»Š'),
            (0x2F9F, 'è¾›'),
            (0x2FA0, 'è¾°'),
            (0x2FA1, 'è¾µ'),
            (0x2FA2, 'é‚‘'),
            (0x2FA3, 'é…‰'),
            (0x2FA4, 'é‡†'),
            (0x2FA5, 'é‡Œ'),
            (0x2FA6, 'é‡‘'),
            (0x2FA7, 'é•·'),
            (0x2FA8, 'é–€'),
            (0x2FA9, 'é˜œ'),
            (0x2FAA, 'éš¶'),
            (0x2FAB, 'éš¹'),
            (0x2FAC, 'é›¨'),
            (0x2FAD, 'é’'),
            (0x2FAE, 'é'),
            (0x2FAF, 'é¢'),
            (0x2FB0, 'é©'),
            (0x2FB1, 'éŸ‹'),
            (0x2FB2, 'éŸ­'),
            (0x2FB3, 'éŸ³'),
            (0x2FB4, 'é '),
            (0x2FB5, 'é¢¨'),
            (0x2FB6, 'é£›'),
            (0x2FB7, 'é£Ÿ'),
            (0x2FB8, 'é¦–'),
            (0x2FB9, 'é¦™'),
            (0x2FBA, 'é¦¬'),
            (0x2FBB, 'éª¨'),
            (0x2FBC, 'é«˜'),
            (0x2FBD, 'é«Ÿ'),
            (0x2FBE, 'é¬¥'),
            (0x2FBF, 'é¬¯'),
            (0x2FC0, 'é¬²'),
            (0x2FC1, 'é¬¼'),
            (0x2FC2, 'é­š'),
            (0x2FC3, 'é³¥'),
            (0x2FC4, 'é¹µ'),
            (0x2FC5, 'é¹¿'),
            (0x2FC6, 'éº¥'),
            (0x2FC7, 'éº»'),
            (0x2FC8, 'é»ƒ'),
            (0x2FC9, 'é»'),
            (0x2FCA, 'é»‘'),
            (0x2FCB, 'é»¹'),
            (0x2FCC, 'é»½'),
            (0x2FCD, 'é¼'),
            (0x2FCE, 'é¼“'),
            (0x2FCF, 'é¼ '),
            (0x2FD0, 'é¼»'),
            (0x2FD1, 'é½Š'),
            (0x2FD2, 'é½’'),
            (0x2FD3, 'é¾'),
            (0x2FD4, 'é¾œ'),
            (0x2FD5, 'é¾ '),
        ];

        for (code_point, standard_char) in kangxi_data {
            if let Some(kangxi_char) = char::from_u32(code_point) {
                let kangxi_str = kangxi_char.to_string();
                let standard_str = standard_char.to_string();

                // Skip if this pair is already handled by script conversion
                if !script_pairs.contains(&(kangxi_str.clone(), standard_str.clone()))
                    && !script_pairs.contains(&(standard_str.clone(), kangxi_str.clone()))
                {
                    kangxi_mappings.insert(kangxi_str, standard_str);
                }
            }
        }

        // Save Kangxi mappings
        let path = "data/processed/normalization/kangxi_radicals.json";
        let json = serde_json::to_string_pretty(&kangxi_mappings)?;
        fs::write(path, json)?;
        println!(
            "âœ… Saved {} Kangxi radical mappings to: {}",
            kangxi_mappings.len(),
            path
        );

        Ok(kangxi_mappings)
    }

    // === REMOVED LEGACY METHODS ===
    // The following methods were removed as they're replaced by the new clean separation approach:
    // - process_script_variants (replaced by process_script_conversion_mappings)
    // - process_character_form_mappings (replaced by process_normalization_mappings)
    // - process_kangxi_mappings (replaced by process_kangxi_radicals_clean)
    // - estimate_complexity (replaced by get_iicore_count)
    // - calculate_stats (replaced by inline statistics generation)
}
